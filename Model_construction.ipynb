{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_construction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONfbbJoivtqq",
        "outputId": "97882614-ef1f-49ce-c396-06bce76eaa96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "win = 50 #window size\n",
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GET DATA\n",
        "data = pd.read_csv('/content/drive/My Drive/hackathon/ground_truth/Subject39_1526591202/1526591202.csv',names=['values'])\n",
        "data['values']=data['values'].apply(lambda x:x/1000.0)\n",
        "arr = np.array(data['values'])\n",
        "max_len = len(arr)\n",
        "arr = np.reshape(arr,(1,max_len))\n",
        "jp = pd.read_csv('/content/drive/My Drive/hackathon/ground_truth/Subject39_1526591202/jpeaks.csv',names=['values'])\n",
        "jp['values'] = jp['values'].apply(lambda x:(x-1526591202)*226)\n",
        "j_len = len(jp['values'])"
      ],
      "metadata": {
        "id": "c5t3e20rv0dG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prev_arr = np.ones((1,max_len))*-1 #position of previous Jpeak for each location\n",
        "\n",
        "count = 0\n",
        "val = -1\n",
        "\n",
        "for i in range(0,max_len):\n",
        "    if count + 1 != j_len:\n",
        "        if jp['values'][count+1]>=i and jp['values'][count]<i:\n",
        "            val = jp['values'][count]\n",
        "            count = count + 1\n",
        "    prev_arr[0][i]=val"
      ],
      "metadata": {
        "id": "CbQEwsiUwBb1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tuple():\n",
        "    start = np.random.choice(range(0,max_len-win))\n",
        "    x = arr[0][start:start+win]\n",
        "    y = prev_arr[0][start+win-1]\n",
        "    \n",
        "    if y == prev_arr[0][start]:#if both start and end have same prev jpeak then no jpeak inbetween\n",
        "        c = 0\n",
        "        y = (y-start)/win\n",
        "    else:\n",
        "        c = 1\n",
        "        y = (y-start)/win\n",
        "    return x,y,c  "
      ],
      "metadata": {
        "id": "rBlDVsV7wDrT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "#Generate Data\n",
        "def generator(batch_size):\n",
        "    while True:\n",
        "        batchx = []\n",
        "        batchy = []\n",
        "        batchc = []\n",
        "        count=0\n",
        "        \n",
        "        while count != batch_size:\n",
        "            x,y,c = get_tuple()\n",
        "            w = random.uniform(0,1)\n",
        "            if (w<=0.72 and c==1) or (w>0.72 and c==0):#This is done to balance the two training classes\n",
        "              batchx.append(x)\n",
        "              batchy.append(y)\n",
        "              batchc.append(c)\n",
        "              count = count + 1\n",
        "        \n",
        "        batchx = tf.convert_to_tensor(batchx)\n",
        "        batchy = tf.convert_to_tensor(batchy)\n",
        "        batchc = tf.convert_to_tensor(batchc)\n",
        "\n",
        "        batchx = tf.cast(batchx,dtype='float32')\n",
        "        batchy = tf.cast(batchy,dtype='float32')\n",
        "        batchc = tf.cast(batchc,dtype='float32')\n",
        "\n",
        "        batchy = tf.reshape(batchy,[batch_size,1])\n",
        "        batchc = tf.reshape(batchc,[batch_size,1])\n",
        "        \n",
        "        yield batchx,batchy,batchc"
      ],
      "metadata": {
        "id": "H4EyfzD8wFyl"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class mlp(layers.Layer):\n",
        "  def __init__(self,units,drop):\n",
        "    super(mlp,self).__init__()\n",
        "    self.units = units\n",
        "    self.drop = drop\n",
        "\n",
        "    self.dense=[]\n",
        "    self.drop_layer=layers.Dropout(self.drop)\n",
        "    for unit in units:\n",
        "      self.dense.append(layers.Dense(unit,activation=tf.nn.gelu))\n",
        "  \n",
        "  def get_config(self):\n",
        "    config = super(mlp,self).get_config()\n",
        "    config.update({\"drop\":self.drop,\"units\":self.units})\n",
        "    return config\n",
        "  \n",
        "  def call(self,x):\n",
        "\n",
        "    for i in range(len(self.units)):\n",
        "      x = self.dense[i](x)\n",
        "      x = self.drop_layer(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "NBCuXrndrM5c"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Construction\n",
        "leaky = layers.LeakyReLU(0.2)\n",
        "inp = layers.Input((win,))\n",
        "x = layers.LayerNormalization()(inp)\n",
        "x = mlp([512,256,128],0.2)(x)\n",
        "y = mlp([128,128],0.1)(x)\n",
        "y=layers.Dense(1,activation='sigmoid')(y)\n",
        "c = mlp([128,128],0.1)(x)\n",
        "c=layers.Dense(1,activation='sigmoid')(c)\n",
        "encoder = tf.keras.Model(inputs=inp,outputs=[y,c])"
      ],
      "metadata": {
        "id": "SUtONnCS08ta"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Training loop for model\n",
        "class trainer(tf.keras.Model):\n",
        "\n",
        "\n",
        "  def __init__(self,mod):\n",
        "    super(trainer,self).__init__()\n",
        "    self.encoder=mod\n",
        "\n",
        "  def compile(self,optim):\n",
        "    super(trainer,self).compile()\n",
        "    self.optim=optim\n",
        "    self.loss = tf.keras.metrics.Mean(name='loss')\n",
        "    \n",
        "  def train_step(self,data):\n",
        "\n",
        "    x,y_true,c_true = data\n",
        "        \n",
        "    with tf.GradientTape() as tape:\n",
        "            \n",
        "        x,y_true,c_true = data\n",
        "        y_preds,c_preds = self.encoder(x)\n",
        "        \n",
        "        z = tf.ones_like(y_true)\n",
        "        reg_loss = tf.reduce_mean(tf.keras.losses.huber(y_true,y_preds)*c_true) \n",
        "        cls_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(c_true,c_preds))\n",
        "            \n",
        "        tot_loss = 0.1*reg_loss+cls_loss\n",
        "\n",
        "    grads=tape.gradient(tot_loss,self.trainable_weights)\n",
        "    self.optim.apply_gradients(zip(grads,self.trainable_weights))\n",
        "    self.loss.update_state(tot_loss)\n",
        "        \n",
        "    return {'loss':self.loss.result()}\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [self.loss]\n"
      ],
      "metadata": {
        "id": "Wr7lbI9swLD7"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model compilation and training\n",
        "final_model = trainer(encoder)\n",
        "opt=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "final_model.compile(opt)\n",
        "final_model.fit(generator(64),steps_per_epoch=40,epochs=100,verbose=1)"
      ],
      "metadata": {
        "id": "AMPXvWhQfEv9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}